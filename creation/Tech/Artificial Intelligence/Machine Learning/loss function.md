A loss function quantifies the error, also known as [[loss]], between the output of a [[neural network]] and intended output.

If the prediction of a model is completely off, the loss function will output a higher number. If the prediction is fairly accurate, the loss function will output a lower number.

It serves as a gauge to tell if our [[neural network]] is improving or not.

Some types of loss functions include
- [[mean squared error]]
- [[binary cross entropy]]
- [[categorical cross entropy]]
- [[huber loss]]