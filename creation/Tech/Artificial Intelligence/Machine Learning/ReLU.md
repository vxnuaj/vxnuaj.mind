---
Last Updated: 03-09-2024 | 5:01 PM
---
ReLU is an [[activation function]] which returns 0 if it receives any negative input, but when given a positive value $x$, it returns the same value back.

It can be defined as 

$f(z) = max(0,z)$

where $z$ is the input value fed into the activation function, which is typically the [[weighted sum]] of a neural network.

![[ReLU.png| Drawn out, it looks like this | 450]]
